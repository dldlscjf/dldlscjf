-기본적인 R 다루기

-변수 입력/생성: (변수명) <- (넣을 내용) 단, 변수명은 문자로 시작해야 한다. 

-저장: Ctrl+S

-새 스크립트 만들기: Ctrl+Shift+N

-필요 패키지 설치: install.packages() 주로 readxl, ggplot2, dplyr 을 설치

-패키지 불러오기: library() 패키지를 설치한 후 함수를 이용하기 위해 불러와야 한다.

-변수 삭제: rm(변수명)

-파일 불러오기/저장하기

-read\_excel(“파일명.xlsx”, col\_names=T/F, sheet=n): 엑셀 파일을 불러온다. 프로젝트 폴더가 아닌 다른 폴더에 있는 엑셀 파일을 불러오려면 파일명이 아닌 파일 경로를 입력해야 한다. 첫번째 행이 변수가 아니라면 col\_names=F 를 입력하면 된다. 여기서 sheet 파라미터를 이용하면 몇번째 시트의 데이터를 불러올지 정한다.

-read.csv(“파일명.csv”,header=T/F): 패키지 설치 없이 CSV파일을 불러오는 함수. 첫번째 행에 변수명이 없는 경우, header=F 파라미터를 지정하면 된다.

-write.csv(df명, file=”파일명.csv”): 데이터 프레임을 CSV파일로 저장

-saveRDS(df명, file=”파일명.rds”): 데이터 프레임을 rds파일로 저장

-readRDS(“파일명.rds”): rds 파일을 불러

-read.spss(file=”파일명.sav”): foreign 패키지의 함수로 spss파일을 불러온다.

-기호 모음

-논리 연산자

|<|<=|>|>=|==|!=|||&|%in%|
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
|작다|작거나 같다|크다|크거나 같다|같다|<p>같지 </p><p>않다</p>|또는|그리고|<p>매칭</p><p>확인</p>|
-산술 연산자

|+|-|\*|/|^, \*\*|%/%|%%|
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
|더하기|빼기|곱하기|나누기|제곱|나눗셈의 몫|나눗셈의 나머지|
-코드 모음

-c(): 변수에 여러 개의 값을 넣을 때 이용

Ex) a<-c(1,2,3) 숫자 대신 문자와 같은 다른 형식의 값도 넣을 수 있다. 문자를 넣을 경우 양 끝에 “(큰 따옴표)를 달아야 한다.

` `a~b 까지의 값을 넣고 싶다면 c(a:b)를 입력

-seq(): 연속 값을 지닌 변수를 만들 때 사용

Ex) seq(1,5) ##[1] 1,2,3,4,5

seq(1,10, by=2) ##[1] 1,3,5,7,9 2 간격 연속 값으로 변수를 생성

-mean(변수): 변수가 가진 값들의 평균을 냄

-max(),min(): 변수의 최댓값, 최솟값을 출력

-sd(): 변수의 표준편차를 구한다.

-paste(): 문자형으로 된 변수들을 붙여서 출력

` `()안에 collaps=” “를 넣으면 “ “안의 값을 구분자로 합침

Ex) paste(str5, collapse=”,”) ##[1]”Hello!, World, is ,good!”

-ifelse(변수(조건), 조건이 맞을 때 부여할 값, 조건이 틀릴 때 부여할 값): 이 때, 조건이 틀릴 때 부여할 값에 ifelse 함수를 한번더 사용하면서 조건1이 틀릴 때 한번 더 나누는 식으로 조건을 늘릴 수 있다. ![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.001.png)

-table(df명$변수): 변수 값들의 빈도표를 생성 

-factor(): 변수의 타입을 factor 타입으로 변환

-데이터 프레임 이해

-행(column): 가로로 나열되는 줄, 열(row): 세로로 나열되는 줄

-data.frame(변수1, 변수2, 변수3): 변수들을 세로로 붙여 데이터 프레임을 만듦.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.002.png) 

`  `이용: mean(df\_midterm$english): df\_midterm의 변수 english의 평균을 구함.

(df명$df의 변수명)형식으로 $를 이용하면 데이터프레임 안에 있는 변수를 지정할 수 있다.

-as.data.frame(패키지 :: 데이터): 데이터의 속성을 데이터 프레임 형태로 바꾸는 함수

(::를 사용하면 특정 패키지에 들어 있는 함수나 데이터를 지정할 수 있다.)

-항상 분석을 할 때 데이터 프레임의 복사본을 만드는 편이 작업 중 오류를 방지하는데 도움이 된다.

-df$추가할변수 <- (추가할 변수의 조건식): 데이터 프레임에 파생변수 만들기

`  `(잘 되었는지 확인하기 위해 View()나 head() 함수를 이용)

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.003.png)                        ![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.004.png)

-to.data.frame=T: SPSS 파일을 데이터 프레임 형태로 변환하는 기능을 한다.

-데이터 가공

-head(변수, n): 데이터 앞에서 n행까지 출력  

-tail(변수, n): 데이터 뒷에서 n행까지 출력   

-dim(변수): 데이터 차원 출력 (행과 열의 개수를 출력)

-str(변수): 데이터 속성 출력 (int: 정수, num: 숫자, chr: 문자        

-summary(): 요약 통계량 출력 (Min, 1st Qu, Median, Mean, 3rd Qu, Max)

-변수 중요도

-klaR 패키지 이용: klaR 패키지는 특정 변수가 주어졌을 때 클래스가 어떻게 분류되는지에 대한 에러율을 계산해주고, 그래픽으로 결과를 보여주는 기능을 한다.

`  `-greedy.wilks(): 세분화를 위한 stepwise forward 변수선택을 위한 패키지, 종속변수에 가장 영향력을 미치는 변수를 wilks lambd를 활용해 변수의 중요도를 정리한다. greedy.wilks(기준으로 잡을 데이터의 변수명~. , data = ?, niveau = 0.1) 여기서 niveau는 F-검정의 유의수준이다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.005.png)

위 명령어로 나온 list를 확인해보면 이렇게 의미있는 변수의 순서대로 표기해준다.

`  `-plineplot(): 특정 변수가 주어졌을 때 기준에 따라 어떻게 분류되는지 error rate를 돌려주고, 그래프를 보여준다. plineplot(기준으로 잡을 데이터의 변수명~. , data=?, method=”lda”, x=?, xlab=(x축명))의 형식으로 그려진다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.006.png)

해석: wine 데이터의 가장 의미있는 변수 V7의 분포를 확인하는 그래프이다. 그림을 보면 0~1에는 class3이 분포, 1~2에는 class2가 분포, 2~3은 class1 과 class2가 함께 분포, 3~에는 class1이 분포한다.

검정: 위의 그래프가 맞는지 검정해본다. 

summary(wine$V7)

Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
`  `0.340   1.205   2.135   2.029   2.875   5.080

V7\_breaks<-c(0,1,2,3,6) summary로 나온 구역을 지정해 주기 위해 이처럼 숫자 벡터 변수를 만든다.

wineV7bin<-cut(wine$V7,V7\_breaks) #cut(데이터,숫자기준): 데이터의 내부를 숫자기준으로 cut하ㅏ고 테이블화한다.

table(wine$class,wineV7bin) #table의 결과로 그래프의 분포와 같다는 것을 확인했다.

wineV7bin
`    `(0,1] (1,2] (2,3] (3,6]
`  `1     0     0    34    25
`  `2     2    31    32     6
`  `3    38    10     0     0

`  `-NaiveBayes(): 변수별 분포를 그래프로 보여준다. NaiveBayes(기준으로 잡을 데이터의 변수명~. , data=?)

Ex) mN<-NaiveBayes(class~. , data=wine[,c(1,8,11,14,2)])

`     `par(mfrow=c(2,2)) 여기서 par(mfrow=c(2,2))는 그래프를 그리기 위한 칸을 정한 것이다. 가로2칸 세로 2칸으로 총 4칸을 설정했다. wine의 변수 중 class와 가장 중요한 변수 4개를 갖고 왔다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.007.png) 

변수별로 그래프를 만들었다. 순서는 왼쪽 위에서 오른쪽 순서로 입력된다.




-변수의 구간화

변수의 구간화에 자주 활용되는 방식은 binning과 의사결정나무가 있다.

-binning: 변수별 분포를 그래프로 보여주고, 연속형 변수를 범주형 변수로 구간화 하는데 자주 활용. Binning은 무조건 해야 하는 것은 아니지만 해석을 용이하게 하는 데 도움이 된다. 그러기 위해서는 Binning 구간을 잘 정하거나 업무 특성에 맞게 해석이나 과거 연구자료와 연관이 되게 구간화해야 한다.

Ex) 위에서 사용한 ‘wine’ 데이터를 활용해 구간화 작업을 해본다.

summary(wine$V7)

Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
`  `0.340   1.205   2.135   2.029   2.875   5.080

wineV7C<-cut(wine$V7,10): wine의 V7을 10으로 binning한다.

10 Levels: (0.335,0.814] (0.814,1.29] (1.29,1.76] ... (4.61,5.08]

출력했을 때 위처럼 10개의 구간으로 나눠진다.

-tree 모형: 의사결정나무 모형 처럼 나무 모양의 그래프로 구간화한 것을 보여주는 모형이다.

Install.packages(“party”), library(party): tree모형을 만들게 해주는 패키지

wine\_ctr<-ctree(class~. , data=wine): wine\_ctr변수에 cut한것의 tree모형을 나타내도록 명령하고, class변수에 대해 모든 변수를 지정한다.

plot(wine\_ctr) 

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.008.png)

해석: root node에서 왼쪽으로 0.335~1.39로 분리되고, 오른쪽으로 1.39~로 분리된다.


-ggplot2 패키지

-qplot(): 변수의 빈도 막대 그래프 출력

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.009.png)

-qplot( data=df명 x=변수1, y=변수2, geom=”유형”, colour=변수n): df의 변수1을 x축, 변수2를 y축으로 “유형”의 그래프를 변수n별로 색을 입혀 표현

-ggplot(data=df명, aes(x=변수1, y=변수2, fill= 변수n))+geom\_유형(): 위와 설명이 같음

-scale\_x\_discrete(limits=c()): x축의 정렬을 c()의 ()안의 순서대로한다.

-dplyr 패키지

-dplyr 패키지 함수들은 변수명 앞에 데이터 프레임명을 반복해 입력하지 않기 때문에 코드가 간결하다는 장점이 있다. => 데이터 프레임명이 길수록, 코드를 구성하는 함수가 많을수록 dplyr 패키지의 장점이 나타난다.

-dplyr 패키지의 함수를 사용 및 연결할때는 %>% 를 붙여 연결한다.

([Ctrl+Shit+M]의 단축키로 입력 가능)

-rename(): 변수명을 바꾸는 함수 → rename(df명, 바꿀 변수명=기존 변수명)

-filter(): 원하는 행을 추출 (~의)

filter(변수 기호 조건) 이런식으로 적어 기입한 변수의 조건에 해당하는 행을 추출한다.

`                              `![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.010.png)

기호 &(혹은 ,+엔터)을 사용하여 조건을 나열하면 여러 조건을 동시에 충족하는 행을 추출하고

|(버티컬 바)를 사용하면 나열한 조건 중 하나라도 충족하는 데이터를 추출할 수 있다.

filter에서 %in% 기호를 사용하면 코드를 간편하게 작성 가능하다. 

filter(class %in% c(1,3,5)) = filter(class==1 & class==3 & class==5)

-select(): 원하는 열(변수) 추출

select(변수)의 형식으로 입력해 변수에 해당하는 열을 추출한다. 이때, 변수 명 앞에 -를 붙이면 그 변수를 제외한 나머지를 추출한다.

select((-)변수1, (-)변수2, (-)변수3, …) 형식으로 여러 개의 변수를 추출할 수 있다.

-arrange(변수): 변수를 낮은 사람에서 높은사람으로 오름차순 정렬

내림차순으로 정렬하려면 arrange(desc(변수))를 입력하면 된다. 

()안에 변수를 여러 개 입력하여 여러 변수를 정렬할 수 있다.

-mutate(추가할 변수명=공식): 기존 데이터에 파생 변수 추가 (~으로, 없던 변수의 생성)

df%>%mutate(추가할 변수명=공식, 변수명2=공식, …) 의 형식으로 기존의 데이터프레임에 변수를 추가한다.

mutate에 ifelse를 추가할 수 있다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.011.png)

Mutate(pct= round(변수 계산식, n): 계산식에 따라 나온 값을 n번째만큼 반올림해 pct라는 파생변수를 추가.

-summarise(): 통계치 산출, 요약 (~으로/~을)

보통 group\_by() 함수와 함께 쓰이며 집단 별 평균/집단 별 빈도 등을 구할 때 이용한다.

df%>%summarise(추가할 변수명=공식)

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.012.png)

-group\_by(): 집단별로 나누기 (~별로)

group\_by()에 여러 변수를 지정하면 집단을 나눈 후 다시 하위집단으로 나눌 수 있다.

다음은 회사(manufacturer)별로 집단을 나눈 후 다시 구동 방식(drv)별로 나눠 도시 연비 평균(cty)을 구하는 코드이다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.013.png)

group\_by(변수)%>%summarise(n = n())을 이용해 변수별로 빈도수를 구할 수 있다.

-dplyr 조합

문제) 회사별로 "suv" 자동차의 도시 및 고속도로 통합 연비 평균을 구해 내림차순으로 정렬하고, 1~5위까지 출력하기  처럼 문제가 나왔을 때

회사별로 / "suv" 자동차의 / 도시 및 고속도로 통합 연비 / 평균을 구해 / 내림차순으로 정렬하고,/  1~5위까지 출력하기/ 와 같이 끊어서 생각하면 코드를 짤 때 비교적 수월하다.

-left\_join(): 데이터 합치기(열을 추가) → 가로로 합치기

left\_join(변수1, 변수2, … , by=”(기준으로 할 변수)”) 로 df을 합친다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.014.png)

-bind\_rows(): 데이터 합치기(행) → 세로로 합치기

Bind\_rows(변수1, 변수2, …) 형태로 이용하며, 합칠 데이터의 변수명이 동일 할 때 합칠 수 있다. 만약 다르다면 rename()을 이용해 동일하게 맞춘 후 합치면 된다. 

-count(df명, 변수명): “df명”의 변수의 빈도수를 셈. count(변수명1, 변수명2, …)형태로 df의 이 변수들의 빈도수를 센다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.015.png)를 count()를 이용해

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.016.png)처럼 간결하게 표현 가능하다.

비율을 구하는 mutate를 하나로 합쳐서 더 간결하게 표현했다.

-데이터 정제

-결측치: 누락된 값, 비어있는 값을 의미한다. 데이터 수집 과정에서 발생한 오류로 결측치를 포함하고 있을 때가 많아 결측치가 있으면 함수가 적용되지 않거나 분석 결과가 왜곡되는 문제가 발생한다. 데이터 프레임에 결측치 만드는 방법: df명[c(NA를 할당할 행 번호)] <- NA

-is.na(): 데이터에 결측치가 있는지를 행열에 따라 TRUE/FALSE 값으로 출력한다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.017.png)

※’is.’으로 시작하는 함수들은 해당 변수가 특정 속성을 지니고 있는지 확인한 후 T/F 값을 반환하는 기능을 갖는다. (Is It ()?)

결측치가 포함된 데이터를 함수에 적용하면 정상적으로 연산되지 않고 NA가 출력된다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.018.png)

-결측치 제거: is.na()를 filter()에 적용하면 결측치가 있는 행을 제거할 수 있다.

df %>% filter(is.na(score)) → score가 NA인 데이터만 출력 → 결측치가 있는 행만 출력

df %>% filter(!is.na(score)) → score가 NA인 데이터를 제외하고 출력 → 결측치를 제거

위 처럼 결측치를 제거하면 수치 연산 함수를 적용할때, 결과가 정상적으로 출력.

-na.omit(): 변수를 지정하지 않고 결측치가 있는 행을 한번에 제거

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.019.png)

단, 이 함수는 간편하지만 분석에 필요한 행까지 손실된다는 단점이 있어 filter()를 이용해 분석에 사용할 변수의 결측치만 제거하는 방식이 좋다.

-na.rm: 수치 연산 함수들의 결측치를 제외하고 연산하도록 설정하는 파라미터

na.rm=T 로 설정하면 결측치를 제외하고 함수를 적용

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.020.png)

-결측치 대체

\1) 평균값으로 대체

변수의 평균을 구하고 ifelse() 함수를 이용해 만약 결측치라면 평균값으로 대체하도록 코드를 짜준다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.021.png)

위 사진은 exam이라는 df의 변수 math의 값이 만약 결측치라면 평균값인 55로 대체하고 아닐 경우 원래의 값을 부여하는 코드이다. 적용시킨 뒤 table()함수를 이용해 결측치의 개수를 확인한다.

FALSE가 20개 나왔으므로 결측치가 없음이 확인되었다.

\2) 통계 분석 기법으로서 각 결측치의 예측값을 추정해 대체하는 방법

-이상치 제거: 데이터에 이상치가 있는지 확인한 후 결측 처리한다.

1\. table() 함수를 이용해 빈도표를 생성하고 있을수 없는 값을 이상치로 생각한다.

2\. Ifelse() 함수를 이용해 만약 변수의 값이 이상치로 나올 수치라면 결측치로 전환하는 코드를 짠다.

Ex) df$a<-ifelse(df$a==3, NA, df$a)

문자일 경우 df$A<-iflese(df$A %in% c(“A”, “B”, “1”), df$A, NA)와 같으 형식으로 결측치로 전환.

3\. 위에서 언급한 filter()를 이용한 결측치 제거 코드로 정제한 뒤 작업을 시작한다.

-상자 그림으로 극단치(이상치) 제거: 극단적으로 크거나 작은 값을 극단치라고 하고 이를 이상치로 인식해 분석하기 전 제거해야 한다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.022.png)

Ifelse() 함수를 이용해 위쪽 극단치 경계와 아래쪽 극단치 경계의 범위를 벗어난 값을 결측 처리한다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.023.png)

이 과정을 거친 후 table()함수를 이용했을 때 결측치가 있다고 나온다면 na.rm이나 filter()를 사용해 결측치를 제거해준다. 

-데이터 분석

-데이터 분석 절차:

1\. 변수 검토 및 전처리: 변수의 특성을 파악하고 이상치를 정제한 다음 파생변수를 만든다. 전처리는 분석에 활용할 변수 각각에 대해 실시한다.

class() 함수로 변수의 타입을 파악하고, table() 함수로 각 범주/수치의 빈도수와 이상치를 확인한다.

단, 연속형 변수는 summary()함수로 요약 통계량을 확인한다. 결측치 확인은 table() 함수로 한다.

Ifelse() 함수로 이상치를 결측 처리하고 결측치를 제거한다.

2\. 변수 간 관계 분석: 데이터를 요약한 표를 만든 후 분석 결과를 이해할 수 있는 그래프를 만든다.

filter(), group\_by(), summarise() 함수를 이용해 요약표를 의미하는 변수를 생성하고 그 변수의 데이터로 그래프를 만든다. 


-그래프 생성

-ggplot2 패키지는 그래프를 만들때 가장 많이 사용하는 패키지이다. 설치하고 시험을 시작하는 것도 괜찮은 방법이다.

-hist(df명$변수): 히스토그램을 생성

-boxplot(df명$변수): 상자 그림 생성

boxplot(df명$변수)$stats 를 입력하면 상자 그림을 만들 때 사용하는 다섯 가지 통계치를 출력한다.

(위에서 아래 순으로 아래쪽 극단치 경계, 1사분위수, 중앙값, 3사분위수, 위쪽 극단치 경계를 의미)

-배경 설정하기: 그래프를 그릴때 배경이 필요한데 여기서 배경이란 x축, y축이 무엇을 나타내는지와 범위를 뜻한다.

ggplot(data=df명, aes(x=변수1, y=변수2) df명을 이용해 df명의 변수1을 x축, 변수2를 y축으로 설정한다는 뜻이다.

-geom\_point(): 산점도 생성

배경을 만드는 코드 뒤에 +geom\_point()를 입력하면 배경에 따른 산점도가 만들어진다.

※’+’는 ggplot2 패키지 함수들을 연결하는데 쓰인다.

ggplot(data=df명, aes(x=변수1, y=변수2)+geom\_point()

-xlim()/ylim(): 축 범위 설정

Ex)xlim(3,6): x축의 범위를 3부터 6까지 설정

-geom\_col(): 막대 그래프 생성, 평균을 비교하는 막대그래프를 만들 때 주로 사용된다.

평균 막대 그래프는 데이터를 요약한 평균표를 먼저 만든 후 평균표를 이용해 만든다.

aes()를 이용해 x과 y축을 설정할 때, reorder()를 사용하면 막대를 크기 순으로 정렬한다.

Ex) ggplot(data= df, aes(x=reorder(x축변수, (-)정렬 기준으로 할 변수), y=y축 변수))+geom\_col()

정렬 기준으로 할 변수 앞에 (-)를 붙이면 내림차순으로 정렬된다.

변수의 값이 숫자와 문자로 함께 구성되어 있으면 숫자 오름차순, 알파벳 오름차순으로 정렬.

geom\_col()의 파라미터를 “dodge”로 설정하면 막대를 분리할 수 있다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.024.png)

coord\_flip()을 이용하면 막대를 오른쪽으로 90도 회전시킨다.

-geom\_bar(): 빈도 막대 그래프를 만들때 이용

빈도 막대 그래프는 별도로 표를 만들지 않고 원자료를 이용해 바로 만든다. 

Ex) ggplot(data=mpg, aes(x=drv))+geom\_bar

x축만 지정하고 geom\_col() 대신 geom\_bar()를 사용한다. y축은 빈도수은 count를 나타내기 때문ㅇ에 따로 지정하지 않아도 상관없다.

x축에 연속 변수를 지정하면 값의 분포를 파악할 수 있다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.025.png)![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.026.png)

-geom\_line(): 선 그래프 생성

`  `-시계열 그래프 만들기:

시계열 데이터를 선으로 만든 그래프를 시계열 그래프라고 한다. 시계열 데이터를 가지고 x축을 시간을 의미하는 변수로 지정하고, y축을 시간에 따른 변화를 보고자 하는 변수로 지정하여 시계열 그래프를 만든다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.027.png)

-geom\_boxplot(): 상자 그림을 생성, 형태는 다른 그래프 생성하는 코드와 비슷하다.

-순서 변수 만들기: 내림차순으로 정렬한 데이터를 담은 변수를 출력하면 

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.028.png)

[1]-[4]-[7] 순서로 그래프의 x축 혹은 y축의 첫번째 기준으로 들어간다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.029.png)

변수를 factor(범주형) 타입으로 변환할때 순서를 지정할 수 있다. factor(바꾸고자 하는 변수, level=c(“a”,”b”,”c”) → 변수의 순서를 a,b,c 순서로 지정

-텍스트 마이닝

문자로 된 데이터에서 가치 있는 정보를 얻어 내는 분석 기법을 ‘텍스트 마이닝’이라고 한다. KoNLP패키지를 이용하면 한글 텍스트의 형태소를 분석 할 수 있다.

-KoNLP 패키지 설치

KoNLP 패키지는 ‘자바’와 rJava 패키지가 설치되어 있어야 사용할 수 있다.

#multilinguer 패키지는 자바와 rJava를 설치하는 install\_jdk를 이용하게 해준다.

install.packages(“multilinguer”) 

library(multilinguer)

install\_jdk()

#KoNLP 패키지의 의존성 패키지들을 설치.

install.packages(c("hash", "tau", "Sejong", "RSQLite", "devtools", "bit", "rex", "lazyeval", "htmlwidgets", "crosstalk", "promises", "later", "sessioninfo", "xopen", "bit64", "blob", "DBI", "memoise", "plogr", "covr", "DT", "rcmdcheck", "rversions"), type = "binary")

#KoNLP 설치 및 불러오기

install.packages("KoNLP")

library(KoNLP)

#형태소 사전 설정하기

useNIADic(): 형태소 분석을 위한 NIA사전을 불러옴

-데이터 준비

-readLines(“문서명.txt”): 프로젝트 파일 안의 텍스트 파일을 불러온다.

-read.csv(“문서명.csv”, header=T, fileEncoding=”UTF-8”): csv 파일을 파일 인코딩을 UTF-8로 바꾼 후 불러 온다. header=T는 첫째 행을 제목으로 할지 정하는 것이다.

-데이터 전처리

txt <- str\_replace\_all(txt, "\\W", " "): [\\W는](\\W는) 역슬래시 두 개와 대문자 W로 이루어진 것이다. 이는 특수문자를 의미하는 정규 표현식이다. 정규 표현식을 이용하면 특정한 규칙으로 되어 있는 부분을 추출할 수 있다. 위 함수를 해석하면 변수 txt의 문장에 들어있는 특수문자를 빈칸으로 수정한다는 뜻이다. “ “는 큰 따옴표를 띄어쓰기 한것임에 유의하자.

gsub("needless word", "", text):text의 필요없는 단어 삭제

install.packages("tidytext")

library(tidytext)

\# tidytext 패키지의 unnest\_tokens(output, input, token(끊어낼 단위))

mango\_df<-data.frame(mango) 분석을 위해 텍스트파일을 데이터 프레임 형태로 바꿔줬다.

mango\_df<-mango\_df %>% 

`  `unnest\_tokens( word, mango, "words") mango 데이터의 word를 “(단어)” 단위로 끊어 냈다.

아래서 하는 명사 추출 대신 이것을 하여 단어 단위로 끊어도 괜찮다. 조사도 제거하고 싶으면 아래의 명사 추출 착업을 하면 된다.

-텍스트 마이닝 추출

extractNoun(변수명): 변수명에서 명사를 추출했다.

#가사에서 명사 추출

nouns <- extractNoun(txt) 

#추출한 명사 list를 문자열 벡터로 변환, 단어별 빈도표 생성

wordcount <- table(unlist(nouns)):  unlist는 list를 문자열 벡터로 변환시켜 준다.

#데이터 프레임으로 변환

df\_word <- as.data.frame(wordcount, stringsAsFactors = F)

※stringAsFactors=F 데이터 프레임을 생성할 때 변수에 문자가 있을 경우 자동으로 factor 타입으로 변환. 하지만 factor 변수는 연산이 되지 않으므로 위 함수로 factor 타입으로 변환되지 않게 한다.

\# 변수명 수정

df\_word <- rename(df\_word, word = Var1, freq = Freq)

\# 두 글자 이상 단어 추출

df\_word <- filter(df\_word, nchar(word) >= 2): 한 글자의 데이터는 별 의미없을 가능성이 크므로 두 글자 이상으로 된 단어만 추출한다.

top\_20 <- df\_word %>% arrange(desc(freq)) %>% head(20)

-워드 클라우드 만들기

wordcloud 패키지를 이용하면 워드 클라우드를 만들 수 있다. 또한 글자 색깔을 표현하는데 사용할 RColorBrewer 패키지를 로드한다.

install.packages("wordcloud")

library(wordcloud)

library(RColorBrewer)

pal<-brewer.pal(8, “Dark2”): Dark2 색상 목록에서 8개 색상 추출

wordcloud()는 함수를 실행할 때 마다 난수를 이용해 매번 다른 모양이 만들어 지므로 난수를 고정한다. set.seed(1234)

wordcloud(words = df\_word$word, # 단어 

`               `freq = df\_word$freq, # 빈도 

`               `min.freq = 2, # 최소 단어 빈도 

`               `max.words = 200, # 표현 단어 수 

`               `random.order = F, # 고빈도 단어 중앙 배치 

`               `rot.per = .1, # 회전 단어 비율 

`               `scale = c(4, 0.3), # 단어 크기 범위 

`               `colors = pal) # 색깔 목록 

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.030.png)

출력된 워드 클라우드는 많이 사용된 단어일수록 글자가 크고 가운데에 배치되며, 덜 사용된 단어일수록 글자가 작고 바깥쪽에 배치되는 형태로 구성된다.

-단어 빈도 막대그래프 만들기

library(ggplot2)

order<-arrange(top20, freq)$word : top20의 word를 freq에 따라 오름차순으로 정렬

ggplot(data= top20, aes(x=word, y=freq))+

`    `ylim(0,2500)+

`    `geom\_col()+

`    `coord\_flip()+

`    `scale\_x\_discrete(limit = order)+              # 빈도순 막대 정렬

`    `geom\_text(aes(label=freq), hjust=0.1)      # 빈도 표시 

geom\_text는 text를 label=?에 따라 표시하고 hjust의 수치로 수평자리를 맞춘다. (0~1)의 값을 가지며 1에 가까울수록 막대 안으로 들어간다.

-Term-Document Matrix: 텍스트마이닝을 위해 불러온 문서에 대해 plain text로 전환, 공백 제거, lowercase로 변환, 불용어 처리, 어간추출 등의 작업을 수행한 다음에 문서번호와 단어 간의 사용 여부 또는 빈도수를 이용해 matrix를 만드는 작업이 term document matrix이다.

`  `-분석 진행

-패키지, 데이터 불러오기

install.packages(“tm”) # 영어처리를 기본으로 하고있다. 한글을 분석하기 위해서는 추가적인 패키지 설치가 필요하다.

library(tm)

data(crude, package="tm")

-matrix 만들기

m<-TermDocumentMatrix(crude, control = list(removePunctuation=T, stopwords=T))

\# removePunctuation: 쉼표, 마침표 제거, stopwords: 불용어 추출

inspect(m) #단어별 문서에서 나온 개수

\> inspect(m)
<<TermDocumentMatrix (terms: 1000, documents: 20)>>
Non-/sparse entries: 1738/18262
Sparsity           : 91%
Maximal term length: 16
Weighting          : term frequency (tf)
Sample             :
`        `Docs
Terms    144 236 237 242 246 248 273 489 502 704
`  `bpd      4   7   0   0   0   2   8   0   0   0
`  `crude    0   2   0   0   0   0   5   0   0   0
`  `dlrs     0   2   1   0   0   4   2   1   1   0
`  `last     1   4   3   0   2   1   7   0   0   0
`  `market   3   0   0   2   0   8   1   0   0   2
`  `mln      4   4   1   0   0   3   9   3   3   0
`  `oil     12   7   3   3   5   9   5   4   5   3
`  `opec    13   6   1   2   1   6   5   0   0   0
`  `prices   5   5   1   2   1   9   5   2   2   3
`  `said    11  10   1   3   5   7   8   2   2   4

-분석 시작

(빈도수가 높은 단어 찾기)

install.packages("slam")

library(slam)

word.count=as.array(rollup(m,2)) #rollup은 집계값을 내주는 함수로 행 합계를 구해 각 단어가 쓰인 횟수(사용빈도)를 구한다.

word.order = order(word.count, decreasing = T) #단어의 사용빈도를 내림차순으로 적어내림

freq.word = word.order[1:30] #가장 많이 쓰이는 단어 30개의 단어 번호를 적음

\> freq.word
` `[1] 609 803 681 613 564 489 147 297 238 539 783 808 985 610 131 485 593
[18] 606 643 680 132 410 697 839 459 548 559 988  84 142

row.names(m[freq.word,]) #가장 많이 쓰인 단어 30개를 뽑아 본다. row.names함수는 ()안에 입력한 값의 행 이름을 출력한다.

\> row.names(m[freq.word,])
` `[1] "oil"        "said"       "prices"     "opec"       "mln"       
` `[6] "last"       "bpd"        "dlrs"       "crude"      "market"    
[11] "reuter"     "saudi"      "will"       "one"        "barrel"    
[16] "kuwait"     "new"        "official"   "pct"        "price"     
[21] "barrels"    "government" "production" "sheikh"     "industry"  
[26] "meeting"    "minister"   "world"      "also"       "billion"

(다른 방법)

findFreqTerms(m,10) # 10개 이상 사용된 단어 표시

\> findFreqTerms(m,10)
` `[1] "barrel"     "barrels"    "bpd"        "crude"      "dlrs"      
` `[6] "government" "industry"   "kuwait"     "last"       "market"    
[11] "meeting"    "minister"   "mln"        "new"        "official"  
[16] "oil"        "one"        "opec"       "pct"        "price"     
[21] "prices"     "production" "reuter"     "said"       "saudi"     
[26] "sheikh"     "will"       "world"     

findAssocs(m, “oil”, 0.7) #”oil” 단어와 70% 이상 연관성이 있는 단어를 표시

\> findAssocs(m, "oil", 0.7) 
$oil
`      `158      opec     named   clearly      late    prices    trying 
`     `0.87      0.87      0.81      0.79      0.79      0.79      0.79 
`   `winter   markets      said  analysts agreement emergency    buyers 
`     `0.79      0.78      0.78      0.77      0.76      0.74      0.71 
`    `fixed 
`     `0.71

-지도 시각화

-패키지 준비

install.packages(“mapproj”

install.packages("ggiraphExtra")

library(ggiraphExtra)

library(tibble)

library(dplyr)

-데이터 준비하기

crime <- rownames\_to\_column(USArrests, var = "state") rownames\_to\_column()을 이용해 행 이름을 state 변수로 바꿔 새 데이터 프레임을 만든다.

crime$state <- tolower(crime$state) tolower() 함수를 이용해 state의 값을 소문자로 수정

-미국 주 지도 데이터 준비

install.packages(“maps”) : 미국 주별 위경도 데이터가 들어있는 maps 패키지를 설치

library(ggplot2) 

states\_map <- map\_data("state") : 지역별 위경도 데이터 표시 

str(states\_map)

-단계 구분도 만들기

ggiraphExtra 패키지의 ggChoropleth()를 이용해 단계 구분도를 만든다.

ggChoropleth(data= crime,               # 지도에 표현할 데이터

`                   `aes=(fill = Murder,       # 색깔로 표현할 변수

`                          `map\_id=state),     # 지역 기준 변수

`                   `map=states\_map)        # 지도 데이터

※인코딩에 따라 오류가 발생할 수도 있으니 incov()를 이용해 형식을 바꾼다.
# incov(데이터, “형식 A”, “형식 B”): 데이터의 형식 A를 B로 바꿈
-인터랙티브 그래프

인터랙티브 그래프란 마우스 움직임에 반응하며 실시간으로 형태가 변하는 그래프이다.

-패키지 준비하기

install.packages("plotly")

library(plotly)

-ggplot2로 그래프 만들기 (산점도)

library(ggplot2)

P<- ggplot(data =mpg, aes(x=displ, y=hwy, col=drv)) + geom\_point()

-인터랙티브 그래프 만들기

ggplotly(p)

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.031.png)

마우스를 드래그하면 특정 영역을 확대할 수 있다.

-인터랙티브 막대 그래프 만들기

P<- ggplot(data = diamonds, aes(x= cut, fill = clarity)) +

`  `geom\_bar(positiion = “dodge”)

ggplotly(p)

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.032.png)

-인터랙티브 시계열 그래프 만들기

install.packages("dygraphs") 

library(dygraphs)

-데이터 준비하기

library(xts) eco <- xts(economics$unemploy, order.by = economics$date) : economics의 date에 따라 unemploy를 xts 타입으로 변경.

dygraphs 패키지의 dygraph()를 이용해 인터랙티브 시계열 그래프를 만든다.

-그래프 만들기

dygraph(eco)

` `![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.033.png)

dygrapheco%>% dyRangeSelctor(): dyRangeSelctor()를 추가하면 그래프 아래에 날짜 범위 선택 기능이 아래에 추가된다.

-여러 값 표현하기

두 데이터를 cbind()를 이용해 가로로 결합하고, xts 타입이기 때문에 rename()(df일 때 이용)을 적용할 수 없으므로 colnames()를 이용해 변수명을 수정

eco2<-cbind(eco\_a, eco\_b)                     # 데이터 결합

colnames(eco2)<-c(“psavert”, “unemploy”) # 변수명 바꾸기

데이터가 준비되었으니 dygraph()를 이용해 그래프를 만든다.

dygraph(eco2) %>% dyRangeSelector()

-t 검정

두 집단의 평균에 통계적으로 유의한 차이가 있는지 알아볼 때 사용하는 통계기법이다. 단일 표본일때도 사용가능하다.

-일표본 t-검정의 가정: 모집단의 구성요소들이 정규분포를 이룬다는 가정 하에 검정통계량의 값을 계산한다.

`  `-가설 설정: 귀무가설: 모평균= 𝜇0, 대립가설: 모평균≠𝜇0

`  `-정규성 검정

-샤피로-윌크 검정: 샤피로-윌크 검정의 귀무가설은 “데이터가 정규분포를 따른다.”이고, 대립가설은 “데이터가 정규분포를 따르지 않는다.”이다. 

Ex) shapiro.test(data)

Shapiro-Wilk normality test

data:  data
W = 0.92173, p-value = 0.2047

`  `-R에서 일표본 t-검정을 수행하기 위해 사용하는 함수는 t.test이며 문법은 아래와 같다.

P-value가 우리가 정한 유의수준 0.05보다 크므로 귀무가설을 기각하지 못해 데이터가 정규분포를 따르고 있음을 알 수 있다.=>t-test를 수행할 수 있다.

t.test(x, alternative=c(“two.sided”,”less”,”greater”),mu=0)

\# x: 표본으로부터 관측한 값(수치형 벡터)

\# alternative: 양측검정시 “two.sided”, 단측검정시 “less”, “greater” 입력

\# mu: 검정 시 기준이 되는 값(𝜇0) t-검정은 평균과 특정 값을 비교하기 때문에 가정하고자 하는 값을 입력한다. 평균이 200쯤 되는것 같다고 추정 ->mu=200으로 설정.

One Sample t-test

data:  data
t = -3.1563, df = 14, p-value = 0.007004
alternative hypothesis: true mean is not equal to 200
95 percent confidence interval:
` `183.2047 196.7953
sample estimates:
mean of x 
`      `190

해석: 검정통계량(t값)은 -3.1563, df(자유도)는 14, 유의확률(p-value)은 0.007004이다. p-value가 유의수준 0.05보다 작기 때문에 귀무가설을 기각하고, 평균이 200g이 아니다.

-대응표본 t-검정(paired sample t-test): 단일모집단에 대해 두 번의 처리를 가했을 때, 두 개의 처리에 따른 평균의 차이를 비교하고자 할 때 사용하는 검정방법이다.

`  `-가설 설정: 귀무가설: 두 개의 모평균 간에는 차이가 없다.

`                 `대립가설: 두 개의 모평균 간에는 차이가 있다.

`  `-R에서 대응표본 t-검정을 수행하기 위해 사용하는 함수는 t.test이며, 문법은 다음과 같다.

t.test(x,y,alternative=c(“two.sided”,”less”,”greater”), paired=F, m=0)

x: 처리방법이 x일 때의 관측값(수치형 벡터)

y: 처리방법이 y일 때의 관측값(수치형 벡터)

alternative: 양측검정시 “two.sided”,  단측검정시 “less”,”greater” 입력

paired: 대응표본 t-검정을 수행할지에 대한 여부 (인자값을 TRUE로 지정해야 한다.)

m: 검정의 기준이 되는 값으로 기본 값은 0이다. 대응표본 t-검정에서는 모평균의 차이가 0인지를 검정하기 때문에, m인자는 적지 않아도 된다.

t.test(data$before, data$after, alternative = "less", paired = T)

`	`Paired t-test

data:  data$before and data$after
t = -4.7434, df = 9, p-value = 0.0005269
alternative hypothesis: true mean difference is less than 0
95 percent confidence interval:
`       `-Inf -0.6135459
sample estimates:
mean difference 
`             `-1

해석: 검정통계량(t값)은 -4.7434, df(자유도)는 9, 유의확률(p-value)은 0.0005269다. 유의수준 0.05보다 작기 때문에 귀무가설을 기각하고, 평균의 차이는 통계적으로 유의하다고 할 수 있다.

-독립표본 t-검정: 두 개의 독립된 모집단의 평균을 비교하고자 할 때 사용하는 검정방법이다.

가설이나 가정은 위의 대응표본 t-검정과 같다. 독립표본 t-검정을 하기에 앞서, 등분산 검정을 수행해야하며 이를 위한 R의 다양한 함수 중 var.test 함수를 이용한다.

var.test(x, y, alternative)

x: 모집단1로부터 측정한 관측값(수치형 벡터)

y: 모집단2로부터 측정한 관측값(수치형 벡터)

alternative: 양측 검정시 “two.sided”, 단측 검정시 “less”, “greater” 입력

F test to compare two variances

data:  temp by group
F = 0.82807, num df = 9, denom df = 9, p-value = 0.7833
alternative hypothesis: true ratio of variances is not equal to 1
95 percent confidence interval:
` `0.2056809 3.3338057
sample estimates:
ratio of variances 
`         `0.8280702

해석: 등분산검정의 결과 유의확률이 0.7833으로 유의수준 0.05보다 크기 때문에 귀무가설을 기각하지 않는다. 따라서 분산의 차이가 없다고 할 수 있고 두 집단의 데이터는 등분산 가정을 만족한다고 볼 수있다. 

`  `-함수: t.test(x,y,alternative, var.equal=T/F)

위의 대응표본 검정과 같지만 var.equal로 등분산성을 만족하는지의 여부만 정하면 된다.

t.test(data=데이터명, 변수a~변수b, var.equal=T): ~기호를 이용해 비교할 값인 a와 비교할 집단인 b변수를 지정. 위의 x,y를 이런식으로 적을 수 있다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.034.png)

해석: 1) 출력된 t 검정 결과에서 ‘p-value’는 유의확률을 의미한다. 일반적으로 유의확률 5%를 판단 기준으로 삼고, p-value가 0.05 미만이면 ‘집단 간 차이가 통계적으로 유의하다‘ 라고 해석.

(유의확률은 실제로는 차이가 없는데 이런 정도의 차이가 우연히 관찰된 확률이 5%보다 작다면, 이 차이르 우연이라고 보기 어렵다고 결론 내리는 것이다.)

위 결과창에서는 p-value<2.2e-16 이므로 유의확률이 2.2 앞에 0이 16개 있는 값보다 작다는 의미이다.

\2) sample estimates 부분을 보면 각 집단의 cty에 대한 평균이 나와있다\. compact의 집단 평균이 suv 보다 높기 때문에 compact의 cty가 더 높다고 할 수 있다\.

` `-상관분석

-상관분석을 위한 R 코드

분산: var(x,y=NULL, na.rm=F)

공분산: cov(x,y=NULL, use=”everything”, method=c(“pearson”,”kendall”,”spearman”)

상관관계: cor(x,y=NULL, use=”everything”, method=c(“pearson”,”kendall”,”spearman”)

※ x=숫자형 변수, y=NULL(default) 또는 변수, na.rm=결측값 처리

`   `상관분석의 가설검정: t 검정통계량을 통해 얻은 p-value 값이 0.05이하인 경우, 대립가설을 채택하        게 되어 데이터를 통해 구한 상관계수를 활용할 수 있게 된다.


-R에 내장된 cor.test()를 이용하면 상관분석을 할 수 있다.

cor.test(변수1, 변수2) => 변수1과 변수2의 상관분석을 한다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.035.png)

해석) 1) p-value가 0.05 미만이므로 실업자 수와 개인 소비 지출의 상관이 통계적으로 유의하다고 해석 할 수 있다.

\2) 결과창 맨 아래의 ‘cor’은 상관계수를 의미한다\. 상관계수가 양수 0\.61이므로, 실업자 수와 개인 소비 지출은 한 변수가 증가하면 다른 변수가 증가하는 정비례 관계임을 알 수 있다\.

-상관행렬 히트맵

cor()을 이용하면 상관행렬을 만들 수 있다. ( cor(데이터) => 데이터 안의 변수별로 상관계수가 나옴.)

여러 변수로 상관행렬을 만들면 너무 많은 숫자로 구성되어 관계를 파악하기 어렵다. 이럴 때 corrplot 패키지의 corrplot()을 이용해 상관행렬을 히트맵으로 만들면 변수들의 관계를 쉽게 파악할 수 있다.

install.packages("corrplot")

library(corrplot)

corrplot(car\_cor)

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.036.png)

출력된 히트맵을 보면 상관계수가 클수록 원의 크기가 크고 색깔이 진하다. 원의 크기와 색깔을 보면 상관관계의 정도와 방향을 쉽게 파악할 수 있다.

Corrplot()의 파라미터를 이용할때 그래프 형태를 다양하게 바꿀 수 있다. method에 “number”를 지정해 원 대신 상관계수가 표현되게 설정.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.037.png)

colorRampPalette()를 이용해 색상 코드 목록을 생성하는 col() 함수를 만들어 파라미터에 활용.

col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))

corrplot(car\_cor, 

`            `method = "color", # 색깔로 표현 

`            `col = col(200), # 색상 200 개 선정 

`            `type = "lower", # 왼쪽 아래 행렬만 표시 

`            `order = "hclust", # 유사한 상관계수끼리 군집화 

`            `addCoef.col = "black", # 상관계수 색깔 

`            `tl.col = "black", # 변수명 색깔 

`            `tl.srt = 45, # 변수명 45 도 기울임 

`            `diag = F) # 대각 행렬 제외

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.038.png)

-분산분석(ANOVA)

분산분석: 두 개 이상의 집단에서 그룹 내 변동에 비교하여 살펴보는 데이터 분석 방법이다. 즉, 두 개 이상 집단들의 평균 간 차이에 대한 통계적 유의성을 검증하는 방법이다.

`  `-가설: 귀무가설: 집단간 모평균에 차이가 없다.(같다)

`          `대립가설: 집단 간 모평균이 모두 같다고 할 수 없다.

※주의사항: R에서 분산분석을 수행할 때 주의해야 할 점은 그룹을 구분하는 기준이 되는 변수는 반드시 factor형이어야 한다. 

R에서 분산분석을 수행하기 위해 사용하는 함수는 aov이며, 문법은 아래와 같다.

aov(formula, data)

formula: 반응변수~그룹변수, data: 분석하고자 하는 데이터명

Ex)iris 데이터로 종별로 꽃받침의 폭의 평균이 같은지 혹은 차이가 있는지를 확인하기 위해 수행. 

`   `result<-aov(Sepal.Width~Species, data = iris) #분산분석의 결과를 result 변수에 저장

`   `summary(result) #분산분석표 작성

\> Df Sum Sq Mean Sq F value Pr(>F)    
Species       2  11.35   5.672   49.16 <2e-16 \*\*\*
Residuals   147  16.96   0.115                   
\---
Signif. codes:  0 ‘\*\*\*’ 0.001 ‘\*\*’ 0.01 ‘\*’ 0.05 ‘.’ 0.1 ‘ ’ 1

해석: 유의수준이 0.05보다 작으므로 귀무가설을 기각한다. 따라서 세가지 종에 따른 꽃받침 폭이 모두 동일하지는 않다고 결론내릴 수 있다. 즉, 종별 꽃받침 폭의 평균값들 중에서 적어도 어느 하나의 종은 통계적으로 유의한 차이가 있는 값을 가진다고 말할 수 있다.

예제에서 평균값들 중에서 적어도 어느 하나의 종은 유의한 차이가 있다고 할 수 있다고 했기 때문에, 세가지 종들 중 특히나 어떠한 종간에 꽃받침의 폭에 차이가 있는지 파악하기 위해 사후검정을 수행.

`  `-사후분석에서는 귀무가설을 ‘집단들 사이의 평균은 같다.’, 대립가설을 ‘집단들 사이의 평균은 같지 않다.’로 두고, 모든 집단 수준에 대해서 두 집단씩 짝을 지어 각각 다중비교를 수행한다.  

`  `-사후검정의 방법 중 R의 TukeyHSD 함수를 이용한 Tukey의 HSD 검정법을 수행할 것이며, TukeyHSD함수의 문법은 다음과 같다.

TukeyHSD(x, conf.level=0.95)

X: 분산분석의 결과, conf.level: 신뢰수준에 해당하며, 기본값은 0.95 이다.

\> TukeyHSD(result)
`  `Tukey multiple comparisons of means
`    `95% family-wise confidence level

Fit: aov(formula = Sepal.Width ~ Species, data = iris)

$Species
`                       `diff         lwr        upr     p adj
versicolor-setosa    -0.658 -0.81885528 -0.4971447 0.0000000
virginica-setosa     -0.454 -0.61485528 -0.2931447 0.0000000
virginica-versicolor  0.204  0.04314472  0.3648553 0.0087802

해석: 세 가지 비교에 대해서 모두 수정된 p-value값이 0.05보다 작으므로 각각의 비교에 대한 귀무가설을 모두 기각한다. 즉 모든 종들에 대해서 꽃받침 폭의 평균값은 각각 통계적으로 유의한 차이가 있다는 것을 알 수 있다. diff는 -의 왼쪽 집단과 오른쪽 집단 간 반응값 차이를 나타내는데, versicolor-setosa에 대한 diff값은 음수이므로, 꽃받침의 폭은 종이 versicolor일 때 보다 setosa일 때가 통계적으로 유의하게 큰 값을 가진다고 볼 수 있다.

-교차분석

교차분석이란 범주형 자료인 두 변수 간의 관계를 알아보기 위해 실시하는 분석 기법이다. 적합도 검정, 독립성 검정, 동질성 검정에 사용된다.

`  `-R을 이용한 적합도 검정

-적합도 검정: 실험에서 얻어진 관측값들이 예상한 이론과 일치하는지 아닌지를 검정하는 방법으로 모집단 분포에 대한 가정이 옳게 됐는지를 관측 자료와 비교하여 검정하는 것이다.

-가설설정: 귀무가설: 실제 분포와 이론적 분포 간에는 차이가 없다.(두 분포가 일치한다.)

`              `대립가설: 실제 분포와 이론적 분포 간에는 차이가 있다.(두 분포가 일치하지 않는다.)

-적합도 검정을 위해 chisq.test 함수를 이용해 카이제곱 검정을 수행한다. 함수는 다음과 같다.

chisq.test(x, y, p=?)

x: 검정하고자 하는 데이터가 저장된 숫자 벡터 혹은 행렬

y: 검정하고자 하는 데이터가 저장된 숫자 벡터 혹은 행렬(x가 행렬일 경우 y인자는 무시된다.)

p: 귀무가설을 통해 설정한 확률을 지정

data(survey, package = "MASS") #MASS 패키지의 survey 데이터를 불러옴

str(survey) #데이터의 특성을 확인

table(survey$W.Hnd) # W.Hnd변수의 분할표 확인 이 변수는 왼손잡이인지 오른손 잡이인지를 뜻함

Left Right 
`   `18   218

data<-table(survey$W.Hnd) # W.Hnd 변수의 분할표를 data변수에 저장 

chisq.test(data, p=c(0.2,0.8)) # 카이제곱검정 수행 여기서 귀무가설을 왼손잡이가 20%, 오른손 잡이가 80%라고 지정했다.

Chi-squared test for given probabilities

data:  data
X-squared = 22.581, df = 1, p-value = 2.015e-06

해석: 유의확률이 0.05보다 작으므로 ‘전체 응답자 중 왼손잡이는 20% 오른손 잡이는 80%라는 귀무가설을 기각한다.

-회귀분석

-회귀분석: 하나나 그 이상의 독립변수들이 종속변수에 미치는 영향을 추정할 수 있는 통계기법이다.

영향을 받는 변수를 종속변수, 영향을 주는 변수를 독립변수라고 한다. 독립변수의 개수가 하나이면 단순선형회귀, 두개 이상이면 다중선형회귀분석으로 분석할 수 있다.

lm(종속변수~독립변수(+독립변수2+독립변수3...))

\> lm(y~x)
Call:

lm(formula = y ~ x)

Coefficients:
(Intercept)            x  
`      `6.409        1.529  

에어컨의 판매대수와 에어컨의 예약대수가 관계가 있는지 회귀분석을 한 예제이다. X는 예약대수이고 y는 판매대수를 뜻한다. 이것만으로는 회귀분석을 검토하기 힘드니 summary()함수로 요약해본다

\> summary(a)

Call:
lm(formula = y ~ x)

Residuals:
`    `Min      1Q  Median      3Q     Max 
-12.766  -2.470  -1.764   4.470   9.412 

Coefficients:
`            `Estimate Std. Error t value Pr(>|t|)    
(Intercept)   6.4095     8.9272   0.718 0.496033    
x             1.5295     0.2578   5.932 0.000581 \*\*\*
\---
Signif. codes:  0 ‘\*\*\*’ 0.001 ‘\*\*’ 0.01 ‘\*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 7.542 on 7 degrees of freedom
Multiple R-squared:  0.8341,	Adjusted R-squared:  0.8104 
F-statistic: 35.19 on 1 and 7 DF,  p-value: 0.0005805

x의 회귀계수인 t통계량에 대한 p-값이 0.000581로 나타나, 유의수준 0.05보다 작으므로 회귀계수의 추정치들이 통계적으로 유의하다. 

결정계수(Multiple R-squared)는 0.8341으로 높게 나타나 이 회귀식이 데이터를 적절하게 설명하고 있다고는 할 수 있다.

결정계수가 높아 데이터의 설명력이 높고 회귀분석결과에서 회귀식과 회귀계수들이 통계적으로 유의하므로 에어컨 판매대수를 에어컨 예약대수로 추정할 수 있다.

다중선형회귀분석도 해보겠다.

\> lm(Price~EngineSize+RPM+Weight, data=Cars93)

Call:
lm(formula = Price ~ EngineSize + RPM + Weight, data = Cars93)

Coefficients:
(Intercept)   EngineSize          RPM       Weight  
` `-51.793292     4.305387     0.007096     0.007271  

\> summary(a)

Call:
lm(formula = Price ~ EngineSize + RPM + Weight, data = Cars93)

Residuals:
`    `Min      1Q  Median      3Q     Max 
-10.511  -3.806  -0.300   1.447  35.255 

Coefficients:
`              `Estimate Std. Error t value Pr(>|t|)    
(Intercept) -51.793292   9.106309  -5.688 1.62e-07 \*\*\*
EngineSize    4.305387   1.324961   3.249  0.00163 \*\* 
RPM           0.007096   0.001363   5.208 1.22e-06 \*\*\*
Weight        0.007271   0.002157   3.372  0.00111 \*\* 
\---
Signif. codes:  0 ‘\*\*\*’ 0.001 ‘\*\*’ 0.01 ‘\*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 6.504 on 89 degrees of freedom
Multiple R-squared:  0.5614,	Adjusted R-squared:  0.5467 
F-statistic: 37.98 on 3 and 89 DF,  p-value: 6.746e-16

해석은 위의 단순선형회귀분석을 해석한 방법과 거의 흡사하다. 가장 영향을 많이 미치는 변수는 EngineSize 임을 알 수 있다.

-최적회귀방정식

필요한 변수만 상황에 따라 타협을 통해 선택해 최적회귀방정식을 만든다. 변수는 단계적 변수선택법을 따르는데 전진선택법, 후진제거법, 단계선택법이 있다. 

\1) 변수 선택법 예제 (유의확률 기반) 선형회귀모형을 생성한 뒤, step()함수를 이용하지 않고 직접 후진제거법을 적용하는 R코드를 작성해 변수제거를 실행

a<-lm(y~x1+x2+x3+x4, data=df)

summary(a)

Call:
lm(formula = y ~ x1 + x2 + x3 + x4, data = df)

Residuals:
`    `Min      1Q  Median      3Q     Max 
-3.1750 -1.6709  0.2508  1.3783  3.9254 

Coefficients:
`            `Estimate Std. Error t value Pr(>|t|)  
(Intercept)  62.4054    70.0710   0.891   0.3991  
x1            1.5511     0.7448   2.083   0.0708 .
x2            0.5102     0.7238   0.705   0.5009  
x3            0.1019     0.7547   0.135   0.8959  
x4           -0.1441     0.7091  -0.203   0.8441  
\---
Signif. codes:  0 ‘\*\*\*’ 0.001 ‘\*\*’ 0.01 ‘\*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.446 on 8 degrees of freedom
Multiple R-squared:  0.9824,	Adjusted R-squared:  0.9736 
F-statistic: 111.5 on 4 and 8 DF,  p-value: 4.756e-07

F통계량은 111.5이고 유의확률이 0.05보다 작아 통계적으로 유의하게 나타났지만 입력변수들의 통계적 유의성을 검토해 본 결과, t 통계량을 통한 유의확률이 0.05보다 작은 변수가 하나도 존재하지 않아 모형을 활용할 수 없다고 판단된다. 유의확률이 가장 높은 x3를 제외하고 다시 생성한다.

b<-lm(y~x1+x2+x4, data = df)

summary(b)

Call:
lm(formula = y ~ x1 + x2 + x4, data = df)

Residuals:
`    `Min      1Q  Median      3Q     Max 
-3.0919 -1.8016  0.2562  1.2818  3.8982 

Coefficients:
`            `Estimate Std. Error t value Pr(>|t|)    
(Intercept)  71.6483    14.1424   5.066 0.000675 \*\*\*
x1            1.4519     0.1170  12.410 5.78e-07 \*\*\*
x2            0.4161     0.1856   2.242 0.051687 .  
x4           -0.2365     0.1733  -1.365 0.205395    
\---
Signif. codes:  0 ‘\*\*\*’ 0.001 ‘\*\*’ 0.01 ‘\*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.309 on 9 degrees of freedom
Multiple R-squared:  0.9823,	Adjusted R-squared:  0.9764 
F-statistic: 166.8 on 3 and 9 DF,  p-value: 3.323e-08

x1을 제외한 2개의 변수가 여전히 유의하지 않은 결과를 보여 유의확률이 가장 높은 x4 변수를 제외하고 회귀모형을 다시 생성한다.

c<-lm(y~x1+x2, data = df)

summary(c)

Call:
lm(formula = y ~ x1 + x2, data = df)

Residuals:
`   `Min     1Q Median     3Q    Max 
-2.893 -1.574 -1.302  1.363  4.048 

Coefficients:
`            `Estimate Std. Error t value Pr(>|t|)    
(Intercept) 52.57735    2.28617   23.00 5.46e-10 \*\*\*
x1           1.46831    0.12130   12.11 2.69e-07 \*\*\*
x2           0.66225    0.04585   14.44 5.03e-08 \*\*\*
\---
Signif. codes:  0 ‘\*\*\*’ 0.001 ‘\*\*’ 0.01 ‘\*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.406 on 10 degrees of freedom
Multiple R-squared:  0.9787,	Adjusted R-squared:  0.9744 
F-statistic: 229.5 on 2 and 10 DF,  p-value: 4.407e-09

F통계량을 통해 유의수준 0.05하에서 모형이 통계적으로 유의함을 확인할 수 있다.

다변량회귀분석에 선정된 x1, x2 변수에 대한 각각의 유의확률 값이 모두 통계적으로 유의하게 나타났다. 이때 후진제거법을 종료하고 회귀식을 얻는다.


2)변수 선택법 에제(벌점화 전진선택법)

step함수를 사용해 전진선택법을 적용하는 R코드를 작성해 변수 제거를 수행한다.

step(lm(출력변수~입력변수, 데이터세트), scope=list(lower=~1, upper=~입력변수), direction=”변수선택방법)

여기서 scope는 변수선택 과정에서 설정할 수 있는 가장 큰 모형 혹은 가장 작은 모형을 설정. 없을 경우 전진선택법에서는 현재 선택한 모형을 가장 큰 모형으로, 후진제거법에서는 상수항만 있는 모형을 가장 작은 모형으로 설정한다.

direction-변수선택법(forward: 전진선택법, backward: 후진제거법, stepwise: 단계적선택법)  

step(lm(y~1, data=df), scope = list(lower=~1, upper=~x1+x2+x3+x4), direction = "forward")

Start:  AIC=71.44
y ~ 1

`       `Df Sum of Sq     RSS    AIC
\+ x4    1   1831.90  883.87 58.852
\+ x2    1   1809.43  906.34 59.178
\+ x1    1   1450.08 1265.69 63.519
\+ x3    1    776.36 1939.40 69.067
<none>              2715.76 71.444

Step:  AIC=58.85
y ~ x4

`       `Df Sum of Sq    RSS    AIC
\+ x1    1    809.10  74.76 28.742
\+ x3    1    708.13 175.74 39.853
<none>              883.87 58.852
\+ x2    1     14.99 868.88 60.629

Step:  AIC=28.74
y ~ x4 + x1

`       `Df Sum of Sq    RSS    AIC
\+ x2    1    26.789 47.973 24.974
\+ x3    1    23.926 50.836 25.728
<none>              74.762 28.742

Step:  AIC=24.97
y ~ x4 + x1 + x2

`       `Df Sum of Sq    RSS    AIC
<none>              47.973 24.974
\+ x3    1   0.10909 47.864 26.944

Call:
lm(formula = y ~ x4 + x1 + x2, data = df)

Coefficients:
(Intercept)           x4           x1           x2  
`    `71.6483      -0.2365       1.4519       0.4161  

벌점화 방식을 적용한 전진선택법을 실시한 결과, 가장 먼저 선택된 변수는 AIC값이 58.852로 가장 낮은 x4였고, x4에 x1을 추가했을 때 AIC 값이 28.742로 낮아지게 되었고, x2를 추가했을 때 AIC값이 24.974로 최소화되어 더 이상 AIC를 낮출 수 없어 변수선택을 종료하고 회귀식을 얻었다.

-로지스틱 회귀분석

로지스틱 회귀분석: 반응변수가 범주형인 경우에 적용되는 회귀분석모형이다. 새로운 설명변수가 주어질 때 반응변수의 각 범주에 속할 확률이 얼마인지를 추정해, 추정 확률을 기준치에 따라 분류하는 목적으로 활용된다.

glm() 함수를 활용하여 로지스틱 회귀분석을 실행한다.

glm(종속변수~독립변수1+…+독립변수k, family=binomial, data=데이터셋명)

Ex) iris 데이터로 로지스틱 회귀분석을 해보겠다. 종속변수 Species가 독립변수 Sepal.Length가 증가함에 따라 어떤 변화를 갖는지 확인해보겠다.

a<-iris[iris$Species=="setosa"|iris$Species=="versicolor",]

b<-glm(Species~Sepal.Length, data = a, family = binomial)

summary(b)

Call:
glm(formula = Species ~ Sepal.Length, family = binomial, data = a)

Deviance Residuals: 
`     `Min        1Q    Median        3Q       Max  
-2.05501  -0.47395  -0.02829   0.39788   2.32915  

Coefficients:
`             `Estimate Std. Error z value Pr(>|z|)    
(Intercept)   -27.831      5.434  -5.122 3.02e-07 \*\*\*
Sepal.Length    5.140      1.007   5.107 3.28e-07 \*\*\*
\---
Signif. codes:  0 ‘\*\*\*’ 0.001 ‘\*\*’ 0.01 ‘\*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

`    `Null deviance: 138.629  on 99  degrees of freedom
Residual deviance:  64.211  on 98  degrees of freedom
AIC: 68.211

Number of Fisher Scoring iterations: 6

해석: Sepal.Length가 한 단위 증가함에 따라 Species(Y)가 1에서 2로 바뀔 때 오즈(Odds)가 exp(5.140)(약 170배) 증가한다.

Null deviance는 절편만 포함하는 모형의 완전 모형이고 (Intercept)의 p-value가 유의수준 0.05보다 작으므로 통계적으로 유의한 적합결여를 나타낸다.

Sepal.Length의 p-value가 유의수준 0.05보다 작으므로 통계적으로 유의한 변수임을 알 수 있고 이 모형은 적합값이 관측된 자료를 잘 적합 한다고 말할 수 있다.

-시계열 분석

install.packages(“tseries”): 시계열 분석을 위한 패키지

install.packages(“forecast”): 자동 시계열 예측 패키지

Install.packages(“TTR”): 분석기술과 데이터가 들어있는 패키지

ts(data): 시계열자료로 만들어주는 함수

plot.ts(시계열 데이터): 시계열 자료를 그래프로 출력

Ex) king.ts<-ts(king)

`     `plot.ts(king.ts)

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.039.png)

\1) 분해 시계열: 시계열에 영향을 주는 일반적인 요인을 시계열에서 분리해 분석하는 방법을 말하며 회귀분석적인 방법을 주로 사용한다\.

SMA(시계열 데이터, n=?): 시계열 데이터를 n회 이동평균 한 값 생성

Ex) 위에서 한 자료를 SMA() 함수를 이용해 n년 마다 평균을 내어 그래프를 표현하겠다.

king.sma3<-SMA(king.ts, n=3) # 3년마다 평균을 내서 그래프를 부드럽게 표현

plot.ts(king.sma3)

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.040.png)

king.sma8<-SMA(king.ts, n=8) # 8년마다 평균을 내서 그래프를 부드럽게 표현

plot.ts(king.sma8)

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.041.png)

2)ARIMA 모델(자기회귀누적이동평균 모델): ARIMA 모델은 정상성 시계열에 한해 사용한다. 비정상 시계열 자료는 차분해 정상성으로 만족하는 조건의 시계열로 바꿔준다. 앞에서 했던 예제의 king.ts 자료의 그래프는 평균이 일정하지 않은 그래프이다. 따라서 차분을 진행한다.

차분은 diff(data, differences=n)의 함수로 진행가능하다.

king.ff1<-diff(king.ts, differences = 1)

plot.ts(king.ff1)

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.042.png)

1차 차분 결과에서 평균과 분산이 시간에 따라 의존하지 않음을 확인 -> 평균과 분산이 일정하다.

따라서 이는 ARIMA(p,1,q) 모델이며 차분을 1번 해야 정상성을 만족한다.

`  `-ACF와 PACF를 통한 적합한 ARIMA 모델 결정

\1) ACF: 자기상관함수로 lag는 0부터 값을 갖는데, 너무 많은 구간을 설정하면 그래프를 보고 판단하기 어렵다\.

acf(데이터, lag.max=n, plot=T/F)

acf(king.ff1, lag.max = 20, plot=F)

Autocorrelations of series ‘king.ff1’, by lag

`     `0      1      2      3      4      5      6      7      8      9 
` `1.000 -0.360 -0.162 -0.050  0.227 -0.042 -0.181  0.095  0.064 -0.116 
`    `10     11     12     13     14     15     16     17     18     19 
-0.071  0.206 -0.017 -0.212  0.130  0.114 -0.009 -0.192  0.072  0.113 
`    `20 
-0.093

acf(king.ff1, lag.max = 20)

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.043.png)

` `그래프를 보면 lag2 부터 점선 안으로 들어가는 그래프인데, 점선 위쪽이 의미있는 값, 아래쪽이 의미없는 값으로 취해진다. 따라서 ACF 값이 lag2에서 절단점을 가진다고 보고 MA(1) 모형이 된다.

2)PACF: 부분자기상관함수를 뜻한다. pacf(데이터명, lag.max=n, plot=T/F)로 표현가능하다.

pacf(king.ff1, lag.max = 20, plot=F)

Partial autocorrelations of series ‘king.ff1’, by lag

`     `1      2      3      4      5      6      7      8      9     10 
-0.360 -0.335 -0.321  0.005  0.025 -0.144 -0.022 -0.007 -0.143 -0.167 
`    `11     12     13     14     15     16     17     18     19     20 
` `0.065  0.034 -0.161  0.036  0.066  0.081 -0.005 -0.027 -0.006 -0.037

pacf(king.ff1, lag.max = 20)

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.044.png)

PACF 값이 lag 1,2,3에서 점선 구간을 초과하고 절단점은 lag4이다. 따라서 AR(3)모형이다.

ARMA(3,1)이 king.ts 자료에서 적절한 모형이라고 할 수 있다.

3)적절한 ARIMA 모형 찾기: 자동으로 찾아주는 함수가 있는데, forecast package에 내장된 auto.arima()함수를 이용하면 된다.

\> auto.arima(king)
Series: king 
ARIMA(0,1,1) 

Coefficients:
`          `ma1
`      `-0.7218
s.e.   0.1208

sigma^2 = 236.2:  log likelihood = -170.06
AIC=344.13   AICc=344.44   BIC=347.56

영국 왕의 사망 나이 데이터의 적절한 ARIMA 모형은 ARIMA(0,1,1) 이다.

4)예측: ARIMA 모형에 의해 보정된 데이터를 통해 미래값을 예측

arima(data, order=c(p,d,q)): 선정된 ARIMA 모형으로 데이터를 보정

forecast(data, h=n): 모형에 의해 보정된 데이터를 통해 미래값을 n회 예측

Ex) > king.arima<-arima(king, order = c(0,1,1))
\> king.forecasts<-forecast(king.arima)
\> king.forecasts
`   `Point Forecast    Lo 80    Hi 80    Lo 95     Hi 95
43       67.75063 48.29647 87.20479 37.99806  97.50319
44       67.75063 47.55748 87.94377 36.86788  98.63338
45       67.75063 46.84460 88.65665 35.77762  99.72363
46       67.75063 46.15524 89.34601 34.72333 100.77792
47       67.75063 45.48722 90.01404 33.70168 101.79958
48       67.75063 44.83866 90.66260 32.70979 102.79146
49       67.75063 44.20796 91.29330 31.74523 103.75603
50       67.75063 43.59372 91.90753 30.80583 104.69543
51       67.75063 42.99472 92.50653 29.88974 105.61152
52       67.75063 42.40988 93.09138 28.99529 106.50596

해석: king.arima는 42명의 영국왕의 사망 나이를 선정된 ARIMA 모형으로 보정한 것이고, king.forecast는 king.arima의 미래를 예측한 값이다. 이것은 43번째에서 52번째 왕까지 10명의 왕의 사망시 나이를 예측한 결과 67.75 살로 추정된다.

-다차원척도법

다차원 척도법이란 군집분석과 같이 개체들을 대상으로 변수들을 측정한 후에 개체들 사이의 유사성/비유사성을 측정하여 개체들을 2차원 공간상에 점으로 표현하는 분석방법이다.

목적: 데이터 속에 잠재해 있는 패턴, 구조를 찾아낸다.

데이터 축소의 목적으로 다차원척도법을 이용한다. 즉, 데이터에 포함되는 정보를 끄집어내기 위해서 다차원척도법을 탐색수단으로써 사용한다.

-계량적 MDS: 데이터가 구간척도나 비율척도인 경우 활용한다. N개의 케이스에 대해서 p개의 특성변수가 있는 경우, 각 객체들간의 유클리드 거리행렬을 계산하고 개체들간의 비유사성 S(거리제곱 행렬의 선형함수)를 공간상에 표현한다.

-분석

\1) cmdscale 사례: cmdscale(data)는 다차원척도분석함수이다\. 2차원으로 도시들을 mapping\.

library(MASS)

loc<-cmdscale(eurodist) # 유럽의 21개 도시의 좌표 데이터를 다차원척도 분석 함수에 넣는다.

x<-loc[,1] # x 좌표를 모아놓은 데이터 

y<- -loc[,2] # 세로축은 북쪽 도시를 상단에 표시하기 위해 부호를 바꾼다.

plot(x, y, type="n", asp=n, main=”???") # asp은 (세로축/가로축의 비율), main은 ???으로 그래프의 이름을 설정하는 것이다.

text(x, y, rownames(loc), cex=0.7) # text(): 그래프에 문자를 입력하는 함수, rownames(data)는 row의 이름을 데이터의 변수로 설정

abline(v=0, h=0, lty=2, lwd=0.5) #abline(): 그래프에 직선을 추가하는 함수, v는 수직선에 대한 x-값, h는 수평선에 대한 y값, lty는 선의 유형 lwd는 선의 두께이다. 유형은 그래프 그리기 항목에서 다뤘다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.045.png)

그래프에서 보이는 지역명이 가까우면 그만큼 거리/유사성이 가깝다는 뜻이다.

-비계량적 MDS: 데이터가 순서척도인 경우 활용한다. 개체들간의 거리가 순서로 주어진 경우에는 순서척도를 거리의 속성과 같도록 변환하여 거리를 생성한 후 적용한다.

\1) isoMDS 사례: 

data(swiss) #스위스연방 중 47개의 불어권 주의 토양의 비옥도 지수와 여러 사회경제적 지표를 측정한 자료이다. 

swiss.x<-as.matrix(swiss[, -1]) # dist 함수를 쓰기 위해 행렬로 변환

swiss.dist<-dist(swiss.x) # dist(): df 또는 행렬의 모든 행 사이의 거리를 계산해 행렬형식으로 반환

swiss.mds<-isoMDS(swiss.dist) # isoMDS(): 비계량 데이터로부터 계산된 거리 행렬에 대해 다차원 척도법을 수행하기 위해서 사용한다.

plot(swiss.mds$points, type="n") 

text(swiss.mds$points, labels = as.character(1:nrow(swiss.x))) #text(x, y, label=?): x.y는 좌표, label은 출력할 문자를 어떻게 할지의 파라미터이다. 이 함수에서는 출력 문자를 글자형태로 숫자 1~swiss.x의 행의 개수만큼 으로 설정했다.

abline(v=0, h=0, lty=2, lwd=0.5)

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.046.png)

가까이 있는 포인트는 거리/유사성이 가깝다는 뜻이다.

2)sammon 사례: sammon()도 위의 isoMDS() 함수처럼 비계량적 MDS 방법이다.

swiss.x<-as.matrix(swiss[,-1])

swiss.sammon<-sammon(dist(swiss.x))

plot(swiss.sammon$points, type="n")

text(swiss.sammon$points, labels = as.character(1:nrow(swiss.x)))

abline(v=0, h=0, lty=2, lwd=0.5)

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.047.png)

-주성분 분석

주성분 분석: ‘주성분’이라는 서로 상관성이 높은 변수들의 선형결합으로 만들어 기존의 상관성이 높은 변수들을 요약, 축소하는 기법이다.

목적: 여러 변수들 간에 내재하는 상관관계, 연관성을 이용해 소수의 주성분으로 차원을 축소함으로써 데이터를 이해하기 쉽고 관리하기 쉽게 해준다. ->데이터를 축소해 이해하기 쉽게 한다.

-분석

1973년 미국 50개주의 100,000명의 인구 당 체포된 세 가지 강력범죄수와 각 주마다 도시에 거주하는 인구의 비율을 나타낸 USArrests 자료를 이용하겠다.

\1) 4개의 변수들 간의 산점도

library(datasets)

data("USArrests")

summary(USArrests)

pairs(USArrests, panel = panel.smooth, main = "USArrests data") #pairs()는 두 개 이상의 변수에 대해 모든 가능한 산점도를 그릴 수 있는 함수이다 pairs(data, panel=panel.smooth, main=”이름“)

해석: murder와 urbanpop 비율간의 관련성이 작아 보인다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.048.png)

2)summary

US.prin<-princomp(USArrests, cor=T) # cor=T를 생략하고 prcomp()로 사용해도 된다. princomp()는 stat 패키지의 함수이다. princomp의 인자 cor은 상관계수 행렬 입력 여부이다.

summary(US.prin) # summary를 하면 주성분 분석의 결과 요약 설명이 나온다.

\> summary(US.prin)
Importance of components:
`                          `Comp.1    Comp.2    Comp.3     Comp.4
Standard deviation     1.5748783 0.9948694 0.5971291 0.41644938
Proportion of Variance 0.6200604 0.2474413 0.0891408 0.04335752
Cumulative Proportion  0.6200604 0.8675017 0.9566425 1.00000000

주성분 분석에서 주성분을 선택하는 방법 중 하나는 분석의 결과에서 누적기여율(Cumulative Proportion)이 85%이상이면 주성분의 수로 결정할 수 있다.

여기서는 제1주성분과 제2주성분까지의 누적 분산 비율은 대략 86.8%로 2개의 주성분 변수를 활용해 전체 데이터의 86.8%룰 설명할 수 있다.

주성분들에 의해 설명되는 변동의 비율은 screeplot을 통해서도 확인 가능하다.

screeplot(US.prin, npcs=4, type = "lines")

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.049.png)

Scree plot을 활용해 고유값이 수평을 유지하기 전 단계로 주성분의 수를 선택 -> Comp.2까지 선택.

3)Loading

Lodadings(US.prin)

\> loadings(US.prin)

Loadings:
`         `Comp.1 Comp.2 Comp.3 Comp.4
Murder    0.536  0.418  0.341  0.649
Assault   0.583  0.188  0.268 -0.743
UrbanPop  0.278 -0.873  0.378  0.134
Rape      0.543 -0.167 -0.818       

`               `Comp.1 Comp.2 Comp.3 Comp.4
SS loadings      1.00   1.00   1.00   1.00
Proportion Var   0.25   0.25   0.25   0.25
Cumulative Var   0.25   0.50   0.75   1.00

네 개의 변수가 각 주성분 Comp.1-Comp.4까지 기여하는 가중치가 제시된다.

제 1주성분에서는 네 개의 변수가 평균적으로 기여한다.

제 2주성분에서는 (Murder, Assault)와 (UrbanPop, Rape)의 계수의 부호가 서로 다르다.

\4) 제 1-2주성분에 의한 행렬도

arrests.pca<-prcomp(USArrests, center = T, scale. = T) #center는 zero 원점 설정 여부, scale은 표준화 여부(변수들의 단위가 다를 경우, 표준화가 바람직함)이다.

biplot(arrests.pca, scale = 0, cex=0.5) #biplot()함수는 제1주성분과, 제2주성분으로 이루어진 좌표평면상의 원 데이터 행들의 주성분점수를 산점도 형태로 나타내고, 각 변수에 대한 주성분계수를 화살표로 시각화하고 그래프로 표현해준다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.050.png)

해석: 범죄를 나타내는 빨간색 화살표에 가까운 지역은 그만큼 그 변수의 비율이 상대적으로 높은 지역이라고 할 수 있다. 반대로 있는 지역은 적다고 해석하면 된다. 

-의사결정나무

의사결정나무: 분류함수를 의사결정 규칙으로 이뤄진 나무 모양으로 그리는 방법이다. 의사결정나무는 주어진 입력값에 대하여 출력값을 예측하는 모형으로 분류나무와 회귀나무 모형이 있다.

-party 패키지를 이용한 의사결정나무

party 패키지는 의사결정나무를 사용하기 편한 다양한 분류 패키지 중 하나이다.

Tree에 투입된 데이터가 표시가 되지 않거나 predict가 실패하는 경우 문제가 발생할 수 있다.

Ex) iris 데이터를 이용해 분석하겠다. 

\1) iris 데이터의 30%는 test data, 70%는 trainig data로 생성한다\.

sample()은 대체를 사용하거나 사용하지 않고 x의 원소에서 지정된 크기의 표본을 추출 하는 함수로 데이터를 분할한다. 

`  `-sample(x, size=?, replace=T/F, prob=NULL) # x: 선택할 하나 이상의 요소의 벡터 또는 양의 정수, size=?: 선택할 항목의 수를 나타내는 음이 아닌 정수, replace=T/F: 샘플링 할때 대체데이터를 사용할지의 여부, prob=NULL: 샘플링 중인 벡터의 요소를 얻기 위한 확률 가중치 벡터

idx<-sample(2,nrow(iris), replace = T, prob=c(0.7,0.3)) # iris의 행의개수를 70%는 1로 30%는 2로 샘플링 한다.

\> idx
`  `[1] 1 1 1 1 1 2 1 2 2 1 1 1 1 1 2 1 1 2 2 1 2 1 2 1 2 1 1 2 1 1 1 1 2 1
` `[35] 1 1 1 1 1 2 2 1 2 2 2 1 1 1 1 1 2 1 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1
` `[69] 1 1 1 2 2 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 2 1 1
[103] 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 2 1 2 1 2 1 1 1 1 2 1
[137] 1 1 2 2 2 2 1 2 1 2 2 1 2 1

train.data<-iris[idx==1,] #위의 idx가 1인 지점의 iris 데이터를 모아둔 데이터

test.data<-iris[idx==2,] #위의 idx가 2인 지점의 iris 데이터를 모아둔 데이터

\2) test\.data를 이용해 모형생성

iris.tree<-ctree(Species~., data=test.data)

plot(iris.tree)

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.051.png)

해석: Petal.Length가 1.9이하와 초과로 나뉘며, 1.9 이하인 그룹은 16개의 원소가 있고 종이 setosa인 데이터의 비율 1.0이다. 1.9 초과인 그룹은 Petal.Length가 4.9이하와 초과로 나뉘며, 4.9 이하인 그룹은 11개의 원소가 있고 종이 versicolor인 데이터의 비율이 약 0.8, virginica인 데이터의 비율이 약 0.2이다. 4.9 초과인 그룹은 12개의 원소가 있고 종이 virginica인 데이터의 비율이 1.0이다.

의사결정나무 그래프를 간단하게 표현할 수 있다.

plot(iris.tree, type=”simple”)

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.052.png)

위의 의사결정나무 그래프는 막대 그래프를 해석해야 했으나 type 파라미터를 simple로 했을 때 숫자를 읽기만 하면 되는 단순한 형태로 나왔다.

\3) 예측된 데이터와 실제 데이터의 비교

table(predict(iris.tree), test.data$Species) #predict() 함수는 주어진 새로운 변수(X)에 대한 예측을 수행한 결과를 예측값(fit – 점추정값에 해당)으로 출력

\> table(predict(iris.tree), test.data$Species)

`             `setosa versicolor virginica
`  `setosa         16          0         0
`  `versicolor      0          9         2
`  `virginica       0          0        12

예측된 데이터와 나무 모형의 데이터가 일치함을 알 수 있다.

\4) train data를 적용하여 정확성 확인

train.pre<-predict(iris.tree, newdata=train.data) # iris.tree는 위에서 test.data로 지정한 모델이지만 newdata 파라미터를 이용해 train.data로 대체한다.

table(train.pre, train.data$Species)

\> table(train.pre, train.data$Species)

train.pre     setosa versicolor virginica
`  `setosa         34          0         0
`  `versicolor      0         39         4
`  `virginica       0          2        32

train.data의 정확성 검정 결과 총 111개가 추출되었고, 위의 test.data는 39개로 원래 iris 데이터의 데이터 개수와 같으므로 잘 분류된 모델이라고 할 수 있다.

-앙상블 분석

앙상블: 주어진 자료로부터 여러 개의 예측모형들을 만든 후 예측모형들을 조합하여 하나의 최종 예측 모형을 만드는 방법

\1) 배깅: 각 붓스트랩 자료에 예측모형을 만든 후 결합해 최종 예측모형을 만드는 방법이다\.

-패키지 설치, 로딩, 데이터 준비

install.packages("adabag") 

library(adabag) 

data(iris) 

-Bagging model 준비

iris.bagging <- bagging(Species~., data=iris, mfinal=10) # mfinal을 통해 10번 반복복원추출하여 학습한다고 명령하였다.

iris.bagging$importance # 변수의 중요도를 확인

\> iris.bagging$importance
Petal.Length  Petal.Width Sepal.Length  Sepal.Width 
`    `78.73731     21.26269      0.00000      0.00000

-그래프화: 분류기의 분류 기준을 보는 것은 plot을 통해 직관적으로 볼 수 있다.

plot(iris.bagging$trees[[10]]) #여기서 [[10]]은 위에서 10번 반복복원추출 했을때 나온 10번째 경우를 뜻한다.

text(iris.bagging$trees[[10]])

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.053.png)

그래프 해석: bagging결과를 tree형태로 표현한 것이다. 변수 중요도 부분에서 봤던 Petal.Length와 Petal.Width가 분류기의 기준으로 작용한 것을 확인할 수 있다.

-모형의 정확도 확인

pred<-predict(iris.bagging, newdata = iris) # iris.bagging의 배깅 모형의 정확성을 알아보기 위해 iris의 예측값을 만든다.

table(pred$class, iris[,5]) # 정오분류표 작성, iris의 5번째 열 Species에 따라 예측값의 class의 빈도를 확인해 보았고, 이정도면 성능이 좋은 편이라고 할 수 있다.

\> table(pred$class, iris[,5])

`             `setosa versicolor virginica
`  `setosa         50          0         0
`  `versicolor      0         47         1
`  `virginica       0          3        49

\2) 부스팅: 예측력이 약한 모형들을 결합해 강한 예측모형을 만드는 방법이다\.

-패키지 라이브러리, 데이터 불러오기

library(adabag) data(iris)

-부스팅 모델 생성

iris.boosting <- boosting(Species~., data=iris, boos=TRUE, mfinal=10) # TRUE인 경우동일하게 bagging처럼 반복 복원추출과정을 거치면서, 각 step에서 잘못 분류된 데이터에 가중치를 부여하여 표본을 추출한다. 기본적으로 TRUE 이다. → Adaboost 기법

iris.boosting$importance # 변수의 중요도를 확인

\> iris.boosting$importance
Petal.Length  Petal.Width Sepal.Length  Sepal.Width 
`   `61.099769    23.135612     7.216024     8.548594

위의 배깅 작업에 비해 Sepal의 중요도가 조금 증가한 것을 알 수 있다.

-그래프화:

plot(boo.adabag$trees[[10]])

text(boo.adabag$trees[[10]])

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.054.png)

그래프 해석: 부스팅 작업이 배깅 작업보다 잎모양으로 분별이 있고 깊이가 더 깊다.

-예측값과 정오분류표

pred <- predict(iris.boosting, newdata = iris) # iris.boosting 데이터의 부스팅 모형의 정확도를 검정하기 위해서 iris의 예측값을 만든다. 

tb <- table(pred$class,iris[,5]) # 정오분류표 작성, iris의 5번째 열 Species에 따라 예측값의 class의 빈도를 확인해 보았고, 위의 배깅 모형보다 더 좋은 성능을 보인다.

\> talbe(pred$class, iris[,5])

`             `setosa versicolor virginica
`  `setosa         50          0         0
`  `versicolor      0         50         0
`  `virginica       0          0        50

\3) 랜덤 포레스트: 배깅과 부스터보다 더 많은 무작위성을 주어 약한 학습기들을 생성한 후 이를 선형 결합하여 최종 학습기를 만드는 방법이다\. 수천 개의 변수를 통해 변수제거 없이 실행되므로 정확도 측면에서 좋은 성과를 보인다\.

-분석

install.packages("randomForest") # 랜덤포레스트 작업은 randomForest 패키지를 설치해야 사용 가능하다.

library(randomForest)

idx<-sample(2, nrow(iris), replace = T, prob = c(0.7,0.3))

idx

table(idx)

train.data<-iris[idx==1,] # 70% 훈련용 데이터 부여

test.data<-iris[idx==2,]  # 30% 시험용 데이터 부여

rf<-randomForest(Species~.,data = train.data, ntree=100, proximity=T)

#ntree: 트리개수, proximity: 행간의 근접성 측정 여부

aa<-table(predict(rf), train.data$Species) #오차율 계산, 행렬의 대각원소가 모델에 의해 정분류된 Case이며 이외는 오분류라고 판단

\> table(predict(rf), train.data$Species)

`             `setosa versicolor virginica
`  `setosa         35          0         0
`  `versicolor      0         33         3
`  `virginica       0          3        31

정<-sum(aa[row(aa)==col(aa)])/sum(aa) #정분류율 계산

[1] 0.9428571

1-정 #오분류율 계산

[1] 0.05714286

rf$confusion #오분류표 만들기

\> rf$confusion
`           `setosa versicolor virginica class.error
setosa         35          0         0  0.00000000
versicolor      0         33         3  0.08333333
virginica       0          3        31  0.08823529

해석: class.error가 추가되어 오류 퍼센트를 보여준다.

plot(rf) #그래프 그리기1 트리 수에 따른 종속변수의 범주별 오분류율을 나타낸다 검은 색은 전체 오분류율을 나타낸다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.055.png)

varImpPlot(rf) #그래프 그리기2 랜덤포레스트에 의해 측정된 변수 중요도를 나타낸 점그래프

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.056.png)

pre.rf<-predict(rf, newdata = test.data) #test data 예측

bb<-table(pre.rf, test.data$Species)

\> table(pre.rf, test.data$Species)

pre.rf       setosa versicolor virginica
`  `setosa         15          0         0
`  `versicolor      0         14         1
`  `virginica       0          0        15

정test<-sum(bb[row(bb)==col(bb)])/sum(bb)

[1] 0.9777778

1-정test

[1] 0.02222222

plot(margin(rf, test.data$Species)) 

#margin은 랜덤포레스트의 분류자 가운데 정분류를 수행한 비율을 다른 클래스로 분류한 비율의 최대치를 뺀 값을 나타낸다. 양의 margin은 정확한 분류를 의미하고, 음의 margin은 오분류를 의미한다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.057.png)

-군집분석

군집분석: 각 객체의 유사성을 측정하여 유사성이 높은 대상 집단을 분류하고, 군집에 속한 객체들의 유사성과 서로 다른 군집에 속한 객체간의 상이성을 규명하는 분석 방법이다.

군집의 개수나 구조에 대한 가정 없이 데이터들 사이의 거리를 기준으로 군집화를 유도한다. 마케팅 조사에서 소비자들의 상품구매행동이나 life style에 따른 소비자군을 분류하여 시장 전략 수립 등에 활용한다.

-계층적 군집분석: n 개의 군집으로 시작해 점차 군집의 개수를 줄여 나가는 방법이다. 계층적 군집을 형성하는 방법에는 합병형 방법과 분리형 방법이 있다.

`  `-분석: iris 데이터를 이용해 계층적 군집분석을 해보겠다.

data(iris)

idx<-sample(1:dim(iris)[1], 40) # 1부터 iris 1열의 차원수(150)를 40개로 나눠 샘플링

\> idx

[1]  85  22 111  50 138  33  18  62  44 129 150  92 142  98  79 133  86
[18] 146  72 109  68 117  25  77 131  65   2  30 121  12  11  49  96 101
[35] 118 139  42  99  51  57

iris.s<-iris[idx,] # iris의 샘플데이터를 만들어준다.

iris.s$Species<-NULL # iris의 샘플데이터에 대한 거리행렬을 만들기 위해 문자형식의 값을 갖고 있는 변수 "Species"는 NULL(없는 값)처리 해준다.

hc<-hclust(dist(iris.s), method = "ave") # method : 클러스터와 관측치와의 거리 계산 기준 - single : 최단거리법 - complete : 최장거리법, 완전기준법 - average : 평균기준법 - median : 중앙중심법

\# dist() 함수는 데이터 행렬의 행 사이의 거리를 계산하기 위해 지정된 거리 측도를 사용하여 계산된 거리 행렬을 계산하고 반환한다. dist(x, method = "euclidean", diag = FALSE, upper = FALSE, p = 2)

plot(hc, hang=-1, labels=iris$Species[idx]) #   # hang : 관측치를 맞춰주기 위한 옵션이다. 

(setosa)와(versicolor,virginica)의 2개의 군집으로 나뉘어 졌다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.058.png)

rect.hclust(hc, k=2) #클러스터를 k기준으로 군집, 보통군집내 변동과 군집간 변동을 이용하여 여러 번 시도를 하여 군집수(k)를 결정 여기서는 k=2로 설정해 2개의 군집으로 나누었다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.059.png)

-k-평균 군집분석: 주어진 데이터를 k개의 클러스터로 묶는 알고리즘으로, 각 클러스터와 거리 차이의 분산을 최소화하는 방식으로 동작한다.

`  `-과정: 원하는 군집의 개수와 초기 값들을 정해 seed중심으로 군집을 형성한다.

`          `각 데이터를 거리가 가장 가까운 seed가 있는 군집으로 분류한다.

`          `각 군집의 seed 값을 다시 계산한다. → 자료들의 평균을 계산해 군집의 중심 갱신

`  `-분석: iris 데이터를 이용해 비계층적 군집방법 중 하나인 k-평균 군집분석을 해보겠다.

\1) 군집화

data(iris)

newiris<-iris

newiris$Species<-NULL

kc<-kmeans(newiris, 3) # k-평균 군집분석때 이용하는 함수로 newiris 라는 데이터를 3개의 군집으로 군집화 한것이다.

\> kc
K-means clustering with 3 clusters of sizes 50, 62, 38

Cluster means:
`  `Sepal.Length Sepal.Width Petal.Length Petal.Width
1     5.006000    3.428000     1.462000    0.246000
2     5.901613    2.748387     4.393548    1.433871
3     6.850000    3.073684     5.742105    2.071053

Clustering vector:
`  `[1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
` `[35] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
` `[69] 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2
[103] 3 3 3 3 2 3 3 3 3 3 3 2 2 3 3 3 3 2 3 2 3 2 3 3 2 2 3 3 3 3 3 2 3 3
[137] 3 3 2 3 3 3 2 3 3 3 2 3 3 2

Within cluster sum of squares by cluster:
[1] 15.15100 39.82097 23.87947
` `(between\_SS / total\_SS =  88.4 %)

Available components:

[1] "cluster"      "centers"      "totss"        "withinss"    
[5] "tot.withinss" "betweenss"    "size"         "iter"        
[9] "ifault"

\2) 결과비교

Table(iris$Species, kc$cluster) # kc의 cluster가 iris의 Species의 무엇에 해당하는지 결과를 비교해본다. 결과를 통해 virginica는 잘 분류하지 못했다고 볼 수 있다.

\> table(iris$Species, kc$cluster)

`              `1  2  3
`  `setosa      0  0 50
`  `versicolor  2 48  0
`  `virginica  36 14  0

\3) 군집화 그래프

plot(newiris[c("Sepal.Length","Sepal.Width")], col=kc$cluster) # newiris 데이터의 Sepal.Length와 Sepal.Width 만 추출하여 종에 따라 색상을 달리함으로써 종의 군집화 그래프를 만든다.

해석: 종에 따라 Sepal.Length 와 Sepal.Width 가 어느정도냐에 따라 분포되어 있는지 한눈에 확인 가능하다.





![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.060.png)

※k-means 군집분석에서 군집 중심의 개수를 결정하는 방법

K-means 군집분석에서는 입력하는 변수와 함께 중심의 개수를 정하는게 중요한데, 몇개의 군집 중심이 적당한지 결정하는 방법에는 여러가지가 있다. 그중 자주 사용하는 NbClust 패키지를 사용하는 방법을 알아보겠다.

\1) NbClust

install.packages("NbClust")

library(NbClust)

nc<-NbClust(newiris, min.nc = 2, max.nc = 20, method = "kmeans") # min.nc: 최소 군집 수 최소한 2개 이상이어야 한다. , max.nc: 최대 군집 수

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.061.png)

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.062.png)

par(mfrow=c(1,1))

barplot(table(nc$Best.nc[1,]), xlab="Number of Clusters", ylab="Number of Criteria", main = "Number of Cluster chosen")

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.063.png)

해석: NbClust의 결론과 막대그래프의 해석으로 알 수 있듯이 2개가 가장 좋다고 보고 있다.

-연관성분석

연관성분석:  기업의 데이터베이스에서 상품의 구매, 서비스 등 일련의 거래 또는 사건들 간의 규칙을 발견하기 위해 적용

-연관규칙의 측도: 산업의 특성에 따라 지지도, 신뢰도, 향상도 값을 잘 보고 규칙을 선택해야 한다.

지지도: 전체 거래 중 항목 A와 항목 B를 동시에 포함하는 거래의 비율로 정의한다.

신뢰도:  항목 A를 포함한 거래 중에서 항목 A와 항목 B가 같이 포함될 확률이다. 연관성의 정도를 파악할 수 있다.

향상도: A가 구매되지 않았을 때 품목 B의 구매확률에 비해 A가 구매됐을 때 품목 B의 구매확률의 증가 비이다. 서로 관련없는 경우 향상도가 1이 된다.

-예제

“arules” 패키지의 Groceries 데이터셋은 식료품 판매점의 1달 동안의 POS 데이터이며, 총 169개의 제품과 9835건의 거래건수를 포함하고 있다.(이미 정제되어 있어 컴퓨터가 읽을 수 있는 언어로 표현되어 있다.) 거래내역을 inspect 함수로 확인할 수 있으며, apriori함수로 최소지지도와 신뢰도는 각각 0.01, 0.3으로 설정한 뒤 연관규칙분석을 실시했다.

-데이터 확인

install.packages("arules")

library(arules)

data(Groceries)

inspect(Groceries[1:3]) # inspect() 함수는  거래데이터의 기록을 갖고와준다. 여기서는 3개의 거래 내역을 갖고 왔다.

\> inspect(Groceries[1:3])
`    `items                 
[1] {citrus fruit,        
`     `semi-finished bread, 
`     `margarine,           
`     `ready soups}         
[2] {tropical fruit,      
`     `yogurt,              
`     `coffee}              
[3] {whole milk}

summary(Groceries)

\> summary(Groceries)
transactions as itemMatrix in sparse format with
` `9835 rows (elements/itemsets/transactions) and
` `169 columns (items) and a density of 0.02609146 

most frequent items:
`      `whole milk other vegetables       rolls/buns             soda 
`            `2513             1903             1809             1715 
`          `yogurt          (Other) 
`            `1372            34055 

element (itemset/transaction) length distribution:
sizes
`   `1    2    3    4    5    6    7    8    9   10   11   12   13   14 
2159 1643 1299 1005  855  645  545  438  350  246  182  117   78   77 
`  `15   16   17   18   19   20   21   22   23   24   26   27   28   29 
`  `55   46   29   14   14    9   11    4    6    1    1    1    1    3 
`  `32 
`   `1 

`   `Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
`  `1.000   2.000   3.000   4.409   6.000  32.000 

includes extended item information - examples:
`       `labels  level2           level1
1 frankfurter sausage meat and sausage
2     sausage sausage meat and sausage
3  liver loaf sausage meat and sausage

해석: itemMatrix in sparse format 은 아이템 목록의 0의 값을 갖지 않은 매우 큰 행렬을 의미한다.

element(itemset/transaction)은 물품 개수에 따른 거래 수를 나타낸다. 1개를 산 거래 수는 2159건, 2개를 산 거래 수는 1643건이다.

-그래프로 확인

itemFrequencyPlot(Groceries, topN=20)

가장 많이 담은 물건의 개수부터 20개를 확인해 달라고 한 그래프이다.

![](Aspose.Words.8a8873f3-f755-4016-9c3c-f95d95b6d8ed.064.png)

-연관성 분석 시작

rules<-apriori(Groceries, parameter = list(support=0.01, confidence=0.3)) apriori 함수는 arules 패키지의 함수로 최소지지도와 신뢰도를 설정한 뒤 연관규칙분석을 실시하게 해준다. 여기서는 지지도를 0.01, 신뢰도를 0.3으로 설정하였다.

\> rules<-apriori(Groceries, parameter = list(support=0.01, confidence=0.3))
Apriori

Parameter specification:
` `confidence minval smax arem  aval originalSupport maxtime support minlen
`        `0.3    0.1    1 none FALSE            TRUE       5    0.01      1
` `maxlen target  ext
`     `10  rules TRUE

Algorithmic control:
` `filter tree heap memopt load sort verbose
`    `0.1 TRUE TRUE  FALSE TRUE    2    TRUE

Absolute minimum support count: 98 

set item appearances ...[0 item(s)] done [0.00s].
set transactions ...[169 item(s), 9835 transaction(s)] done [0.00s].
sorting and recoding items ... [88 item(s)] done [0.00s].
creating transaction tree ... done [0.00s].
checking subsets of size 1 2 3 4 done [0.00s].
writing ... [125 rule(s)] done [0.00s].
creating S4 object  ... done [0.00s].

분석 해석: parameter specification에 최소지지도와 신뢰도가 표시된다. 마지막 문단에 set transactions에서 169개의 아이템이 9835개의 거래건수를 갖고 있음을 알 수 있고, sorting and recoding items에서 88개의 아이템으로 연관규칙을 만들었음을 알 수 있으며, writing에서 보이는 125 rule(s)는 125개의 규칙이 발견되었다는 뜻이다. 규칙의 수가 너무 적으면 지지도와 신뢰도를 낮추고, 너무 많으면 지지도와 신뢰도를 높여야 한다.

inspect(sort(rules,by=c("lift"),decreasing = T)[1:20]) # apriori 함수로 연관성분석을 한 데이터에 대해 inspect 함수를 사용하면 생성된 연관규칙을 확인할 수 있다. 여기서는 rules라는 연관규칙 데이터를 lift(향상도)로 구분하고 내림차순으로 20개를 출력하였다.

\> inspect(sort(rules,by=c("lift"),decreasing = T)[1:20])
`     `lhs                     rhs                   support confidence   coverage     lift count
[1]  {citrus fruit,                                                                            
`      `other vegetables}   => {root vegetables}  0.01037112  0.3591549 0.02887646 3.295045   102
[2]  {tropical fruit,                                                                          
`      `other vegetables}   => {root vegetables}  0.01230300  0.3427762 0.03589222 3.144780   121
[3]  {beef}               => {root vegetables}  0.01738688  0.3313953 0.05246568 3.040367   171
[4]  {citrus fruit,                                                                            
`      `root vegetables}    => {other vegetables} 0.01037112  0.5862069 0.01769192 3.029608   102
[5]  {tropical fruit,                                                                          
`      `root vegetables}    => {other vegetables} 0.01230300  0.5845411 0.02104728 3.020999   121
….....

해석: 향상도를 기준으로 내림차순으로 정렬한 수 상위 5개의 규칙을 확인해봤을 때, rhs의 제품만 구매할 확률에 비해 lhs의 제품을 샀을 때 rhs 제품도 구매할 확률이 약 3배 가량 높다(lift>3 이기 때문에). 따라서 rhs와 lhs 제품들간 결합상품 할인쿠폰 혹은 품목배치 변경 등을 제안할 수 있다.

※ 지지도와 신뢰도를 설정하지 않으면 lhs(left hand side)에 해당하는 항목이 없는 경우에도 rhs의 결과가 나타나는 무의미한 경우가 있을 수 있기 때문에 조정하여 발굴해야 한다.

※ 만약 컴퓨터가 인식 못하는 상태의 데이터라면, 즉, 수치형이 아닌 문자형 데이터라 연관성 분석이 힘든 상태라면 데이터를 spare matrix로 만들어서 수치데이터로 변경해야 한다.

Ex) dataset<-read.transaction(“data.csv”, sep=”,”, rm.duplicate = T) 은 rm.duplicate는 중복된 것을 제거해준다. 이 함수를 입력하면 data.csv의 등장하는 모든 물품을 칼럼으로 만들고, 구매한 기록은 해당 컬럼에 1이라고 쓰고 구매 안했으면 0으로 입력한다.

-사회연결망 분석

사회연결망 분석(SNA): 개인과 집단들 간의 관계를 노드와 링크로 모델링하여 그것의 위상구조와 확산 및 진화 과정을 계량적으로 분석하는 방법론.
